{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0161ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "dataset_type = ['train', 'val']\n",
    "\n",
    "def dataset_generator(dataset_type, patch_size, scaling_factor):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    roots = [\"Y2B_23\", \"Y2B_24\"]\n",
    "    for ds in roots:\n",
    "        if ds == \"Y2B_23\":  # Y2B_23 is the only one with train/val subfolders in images\n",
    "            image_dir = f\"../Task_2/datasets/Y2B_23/{ds}/{dataset_type}\"\n",
    "            images_path = sorted(glob.glob(f\"{image_dir}/*.png\"))\n",
    "        else:\n",
    "            image_dir = f\"../Task_2/datasets/{ds}/images\"\n",
    "            all_images = glob.glob(f\"{image_dir}/*.png\")\n",
    "            images_path = sorted(\n",
    "                p for p in all_images\n",
    "                if os.path.basename(p).startswith(f\"{dataset_type}_\")\n",
    "            )\n",
    "        \n",
    "        mask_root = f\"../Task_2/datasets/{ds}/masks\"\n",
    "\n",
    "        for image_path in images_path:\n",
    "            base = os.path.basename(image_path)\n",
    "            stem, _ = os.path.splitext(base)\n",
    "\n",
    "            if ds == \"Y2B_23\":\n",
    "                root_candidates = os.path.join(mask_root, stem + \"_root_mask.tif\")\n",
    "                if not root_candidates:\n",
    "                    continue\n",
    "            else:\n",
    "                root_candidates = glob.glob(os.path.join(mask_root, \"*\", stem + \"_root_mask.tif\"))\n",
    "                if not root_candidates:\n",
    "                    continue\n",
    "                root_mask_path = root_candidates[0]\n",
    "\n",
    "            if not os.path.exists(root_mask_path):\n",
    "                raise FileNotFoundError(f\"Mask not found: {root_mask_path}\")\n",
    "            \n",
    "            img = cv2.imread(image_path)\n",
    "            root_mask = cv2.imread(root_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            if scaling_factor != 1:\n",
    "                img = cv2.resize(img, None, fx=scaling_factor, fy=scaling_factor)\n",
    "                root_mask = cv2.resize(root_mask, None, fx=scaling_factor, fy=scaling_factor)\n",
    "                \n",
    "            height, width = img.shape[:2]\n",
    "\n",
    "            for top in range(0, height - patch_size + 1, patch_size):\n",
    "                for left in range(0, width - patch_size + 1, patch_size):\n",
    "                    img_patch = img[top:top+patch_size, left:left+patch_size, :] # x, y, chanels\n",
    "                    root_patch = root_mask[top:top+patch_size, left:left+patch_size]\n",
    "\n",
    "                    root_patch = root_patch[..., np.newaxis] # \"...\" - Ellipsis - takes all dimensions\n",
    "\n",
    "                    X.append(img_patch)\n",
    "                    y.append(root_patch)\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    # Normalize\n",
    "    X = X/255\n",
    "    y = y/255\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4627425",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 128\n",
    "scaling_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d511f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataset_generator(dataset_type='train',\n",
    "                                     patch_size=patch_size,\n",
    "                                     scaling_factor=scaling_factor)\n",
    "X_train.shape, y_train.shape # ((4680, 128, 128, 3), (4680, 128, 128, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = dataset_generator(dataset_type='val',\n",
    "                                 patch_size=patch_size,\n",
    "                                 scaling_factor=scaling_factor)\n",
    "X_val.shape, y_val.shape # ((520, 128, 128, 3), (520, 128, 128, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script needs these libraries to be installed:\n",
    "#   tensorflow, numpy\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Start a run, tracking hyperparameters\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata with wandb.config\n",
    "    config={\n",
    "        \"layer_1\": 256,\n",
    "        \"activation_1\": \"relu\",\n",
    "        \"dropout\": random.uniform(0.01, 0.80),\n",
    "        \"layer_2\": 10,\n",
    "        \"activation_2\": \"softmax\",\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"loss\": \"sparse_categorical_crossentropy\",\n",
    "        \"metric\": \"accuracy\",\n",
    "        \"epoch\": 8,\n",
    "        \"batch_size\": 256\n",
    "    }\n",
    ")\n",
    "\n",
    "# [optional] use wandb.config as your config\n",
    "config = wandb.config\n",
    "\n",
    "# get the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train, y_train = x_train[::5], y_train[::5]\n",
    "x_test, y_test = x_test[::20], y_test[::20]\n",
    "labels = [str(digit) for digit in range(np.max(y_train) + 1)]\n",
    "\n",
    "# build a model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(config.layer_1, activation=config.activation_1),\n",
    "    tf.keras.layers.Dropout(config.dropout),\n",
    "    tf.keras.layers.Dense(config.layer_2, activation=config.activation_2)\n",
    "    ])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=config.optimizer,\n",
    "              loss=config.loss,\n",
    "              metrics=[config.metric]\n",
    "              )\n",
    "\n",
    "# WandbMetricsLogger will log train and validation metrics to wandb\n",
    "# WandbModelCheckpoint will upload model checkpoints to wandb\n",
    "history = model.fit(x=x_train, y=y_train,\n",
    "                    epochs=config.epoch,\n",
    "                    batch_size=config.batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[\n",
    "                      WandbMetricsLogger(log_freq=5),\n",
    "                      WandbModelCheckpoint(\"models\")\n",
    "                    ])\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Y2B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
